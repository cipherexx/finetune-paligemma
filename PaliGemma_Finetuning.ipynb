{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMxXY99kAegM"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install datasets transformers pillow\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visit the huggingface page for paligemma and accept the terms there to use paligemma here."
      ],
      "metadata": {
        "id": "MlhogiVgrDAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iie6uKbiA6ZF"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIrz8JRbGJVt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqakATmHKquc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    PaliGemmaProcessor,\n",
        "    PaliGemmaForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from PIL import Image\n",
        "from peft import get_peft_model, LoraConfig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBbMh9E3KuTF"
      },
      "outputs": [],
      "source": [
        "'''Replace dataset here with any VQA dataset, if the dataset has the same format as VQAv2, no need to change anything,\n",
        "otherwise adjust columns to remove and the question and target column ahead accordingly'''\n",
        "dataset = load_dataset(\"pminervini/VQAv2\")\n",
        "train_ds = dataset['train']\n",
        "\n",
        "# Remove unnecessary columns if any\n",
        "try:\n",
        "    cols_remove = [\"question_type\", \"answers\", \"answer_type\", \"image_id\", \"question_id\"]\n",
        "    train_ds = train_ds.remove_columns([col for col in cols_remove if col in train_ds.column_names])\n",
        "except:\n",
        "    print(\"Some columns don't exist in this dataset, continuing...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEvl4c8SK4JZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import PaliGemmaProcessor\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize the processor for your model\n",
        "model_id = \"google/paligemma-3b-pt-224\"\n",
        "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Define a transform to convert the image to a tensor and resize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 for the model\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def collate_fn(examples):\n",
        "\n",
        "    texts = [f\"<image> <bos> answer {example['question']}\" for example in examples]\n",
        "    labels = [str(example.get('answer', example.get('multiple_choice_answer', ''))) for example in examples]\n",
        "\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        image = example[\"image\"]\n",
        "        image_tensor = transform(image)\n",
        "\n",
        "        # Check if the image is grayscale (1 channel)\n",
        "        if image_tensor.shape[0] == 1:\n",
        "            image_tensor = image_tensor.repeat(3, 1, 1)  # Convert grayscale to RGB by repeating the channel\n",
        "        images.append(image_tensor)\n",
        "\n",
        "    # Process inputs with the processor\n",
        "    model_inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256,      # Adjust max_length as needed\n",
        "        return_tensors=\"pt\",\n",
        "        do_rescale=False  # Disable rescaling if images are already scaled between 0 and 1\n",
        "    )\n",
        "\n",
        "    # Process targets (labels)\n",
        "    with processor.tokenizer.as_target_tokenizer():\n",
        "        label_encoding = processor.tokenizer(\n",
        "            labels,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            # Ensure max_length for labels is consistent with input\n",
        "            max_length=model_inputs[\"input_ids\"].shape[1],\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    # Create attention mask for labels (1 for real tokens, 0 for padding)\n",
        "    labels = label_encoding[\"input_ids\"].clone()\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Replace padding with -100\n",
        "    model_inputs[\"labels\"] = labels\n",
        "\n",
        "    processed_inputs = {}\n",
        "    for k, v in model_inputs.items():\n",
        "        if k in ['input_ids', 'attention_mask', 'labels']:\n",
        "            # Keep these as Long tensors\n",
        "            processed_inputs[k] = v.to(\"cuda\")\n",
        "        else:\n",
        "            # Convert other tensors (like pixel_values) to bfloat16\n",
        "            processed_inputs[k] = v.to(torch.bfloat16).to(\"cuda\")\n",
        "\n",
        "    return processed_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1gPM5DhLAqL"
      },
      "outputs": [],
      "source": [
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_type=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Initialize model with quantization and LoRA\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db5VMi9dLBxS"
      },
      "outputs": [],
      "source": [
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=2,\n",
        "    remove_unused_columns=False,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=1e-6,\n",
        "    adam_beta2=0.999,\n",
        "    logging_steps=100,\n",
        "    optim=\"adamw_hf\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=10,\n",
        "    push_to_hub=False,\n",
        "    save_total_limit=1,\n",
        "    output_dir=\"/content/out\",\n",
        "    bf16=True,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    dataloader_pin_memory=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBqmDohhLF_u"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    data_collator=collate_fn,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFZPd84B4lYF"
      },
      "outputs": [],
      "source": [
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_type=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1BcAheF8Osd"
      },
      "outputs": [],
      "source": [
        "from safetensors.torch import load_file\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
        "from PIL import Image\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzbDDzBy8SBi"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_checkpoint_path = \"/content/out/checkpoint-250/adapter_model.safetensors\"  # Model path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X850hXoP8UvE"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/paligemma-3b-mix-224\"  # Base model name\n",
        "processor = AutoProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ZGpajx8YxM"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRgoYzIw8eZs"
      },
      "outputs": [],
      "source": [
        "state_dict = load_file(model_checkpoint_path)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xkg90lf-AreM"
      },
      "outputs": [],
      "source": [
        "image_path = \"./output.jpg\"  # Image file path\n",
        "text_prompt = \"<image> <bos>What is shown in this image ?\"  # Prompt with special tokens\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image=image.resize((32,32), Image.BICUBIC)\n",
        "image=image.resize((224,224), Image.BICUBIC)\n",
        "inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=400)  # Adjust max_length as needed\n",
        "\n",
        "response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Model's Response: {response}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}